# AI 에이전트 배포 전략 가이드

## 개요
수십 개의 AI 에이전트를 배포할 때 Vercel, Cloud Run, 그 외 플랫폼의 비교 분석

---

## 플랫폼 비교

### 1. Vercel
| 항목 | 내용 |
|------|------|
| **장점** | 빠른 배포, 우수한 DX, GitHub 연동, Edge Network |
| **단점** | 대역폭 비용 높음 ($550/TB), 함수 실행시간 제한 |
| **가격** | Hobby 무료, Pro $20/mo |
| **적합** | MVP, 프론트엔드, 경진대회 |

```
비용 예시 (월 100만 요청):
- Pro: $20 + 대역폭 초과분
- 대역폭 1TB 초과 시 급격히 증가
```

### 2. Google Cloud Run
| 항목 | 내용 |
|------|------|
| **장점** | 요청 기반 과금, 자동 스케일링, GCP 생태계 |
| **단점** | 콜드스타트 (2-5초), 설정 복잡 |
| **가격** | 200만 요청/월 무료, 이후 $0.40/백만 |
| **적합** | 서비스화, 대규모 트래픽 |

```
비용 예시 (월 100만 요청):
- 무료 티어로 충분
- 메모리 512MB 기준 약 $5-10/월
```

### 3. Railway
| 항목 | 내용 |
|------|------|
| **장점** | Usage-based, 설정 간편, 자동 스케일링 |
| **단점** | 무료 티어 제한적 |
| **가격** | $5/mo 시작, 사용량 기반 |
| **적합** | 초기 서비스, 스타트업 |

### 4. Render
| 항목 | 내용 |
|------|------|
| **장점** | 간편한 배포, 무료 티어 있음 |
| **단점** | 무료 인스턴스 15분 휴면 |
| **가격** | 무료 ~ $7/mo (Starter) |
| **적합** | 사이드 프로젝트, 테스트 |

### 5. Fly.io
| 항목 | 내용 |
|------|------|
| **장점** | Edge 배포, 글로벌 분산, 빠른 콜드스타트 |
| **단점** | 학습 곡선 |
| **가격** | 3개 VM 무료, 이후 사용량 기반 |
| **적합** | 글로벌 서비스, 저지연 필요 |

---

## 비용 비교 (수십 개 에이전트)

| 플랫폼 | 10개 에이전트 | 50개 에이전트 | 비고 |
|--------|--------------|--------------|------|
| **Vercel** | $20 + α | $100+ | 대역폭 초과 시 급증 |
| **Cloud Run** | $0-20 | $50-100 | 트래픽 비례 |
| **Railway** | $50-100 | $200-400 | 항상 실행 시 |
| **Render** | $70 | $350 | $7 x 개수 |
| **Fly.io** | $0-30 | $100-200 | Edge 최적화 |

---

## 2025년 AI 배포 트렌드

### 1. Unified Runtime (통합 런타임)
마이크로서비스 대신 단일 컨테이너에 여러 에이전트 배포

```
기존 방식:
Agent1 → Container1
Agent2 → Container2
Agent3 → Container3
(컨테이너 수십 개, 관리 복잡)

Unified Runtime:
Agent1, Agent2, Agent3 → 단일 vLLM/BentoML 서버
(모델 공유, 메모리 효율, 관리 간편)
```

**도구:**
- **vLLM**: 고성능 LLM 서빙
- **BentoML**: ML 모델 배포 프레임워크
- **LangServe**: LangChain 에이전트 서빙

### 2. Hybrid Architecture (하이브리드 아키텍처)

```
┌─────────────────────────────────────────────────────┐
│                    사용자 요청                        │
└─────────────────────┬───────────────────────────────┘
                      ▼
┌─────────────────────────────────────────────────────┐
│              Frontend (Vercel/Cloudflare)           │
│                   CDN + Edge Functions              │
└─────────────────────┬───────────────────────────────┘
                      ▼
┌─────────────────────────────────────────────────────┐
│              API Gateway (Cloud Run)                │
│           라우팅, 인증, Rate Limit                    │
└─────────────────────┬───────────────────────────────┘
                      ▼
┌─────────────────────────────────────────────────────┐
│           Agent Orchestrator (K8s/Modal)            │
│         vLLM + LangGraph + 에이전트 풀               │
└─────────────────────────────────────────────────────┘
```

### 3. Modal Labs (신흥 강자)
- GPU 서버리스 플랫폼
- LLM 추론에 최적화
- 초당 과금, 콜드스타트 거의 없음
- AI 에이전트에 이상적

---

## 단계별 권장 전략

### Phase 1: MVP / 경진대회 (현재)
```
✅ Vercel 유지
- 빠른 배포, 간편한 관리
- 비용: 무료 ~ $20/월
- 에이전트 수: 1-5개
```

### Phase 2: 초기 서비스 (사용자 1,000명+)
```
📦 Railway 또는 Cloud Run 전환
- 비용 효율적
- 자동 스케일링
- 에이전트 수: 5-20개
```

### Phase 3: 스케일업 (사용자 10,000명+)
```
🚀 Hybrid Architecture
- Frontend: Vercel (정적 + Edge)
- API Gateway: Cloud Run
- Agent Pool: K8s + vLLM
- 에이전트 수: 20개+
```

### Phase 4: 엔터프라이즈 (대규모)
```
🏢 자체 인프라
- K8s 클러스터
- vLLM / BentoML
- GPU 노드 풀
- 에이전트 수: 100개+
```

---

## MemoMate 권장사항

### 현재 (경진대회)
- **Vercel 유지** - 변경 불필요
- 단일 에이전트, 무료 티어 충분

### 서비스화 시
1. **Phase 1**: Vercel 유지 + Upstash Pro
2. **Phase 2**: Cloud Run 전환 검토
3. **에이전트 추가 시**: Modal Labs 또는 BentoML 고려

### 비용 최적화 팁
1. LLM 호출 캐싱 (Redis)
2. 규칙 기반 분류 우선 (이미 적용됨)
3. 야간 배치 처리
4. 에이전트 통합 (Unified Runtime)

---

## 참고 자료
- [2025 Serverless Comparison](https://intuitionlabs.ai/articles/llm-api-pricing-comparison-2025)
- [Cloud Run vs Vercel](https://cloud.google.com/run)
- [Modal Labs](https://modal.com)
- [BentoML Docs](https://docs.bentoml.com)
- [vLLM Project](https://github.com/vllm-project/vllm)
